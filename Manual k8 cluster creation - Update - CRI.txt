Pre-requisites
---------------

t2.medium with 2 CPUs  and 4 G Ram 
Ubuntu Server 20.04 LTS (HVM), SSD Volume Type - ami-00399ec92321828f5 (64-bit x86) 

A system user with "sudo" access on each server. 
open the below port on each  Instance

6443	TCP	0.0.0.0/0  - Default API  serverport


perform following steps in all nodes
------------------------------------

 Step 1
 ------

set a hostname on each server. After executing the following commands on each server,
 re-login to the servers so that the servers will get a new Hostname. 


sudo hostnamectl set-hostname "nd-ctl-pl.tnh.com"
sudo hostnamectl set-hostname "nd-wk1.tnh.com"
sudo hostnamectl set-hostname "nd-wk2.tnh.com"


ubuntu@nd-ctl-pl:~$ sudo apt update
Hit:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal InRelease
Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
Get:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-backports InRelease [101 kB]
Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease
Fetched 214 kB in 0s (556 kB/s)
Reading package lists... Done
Building dependency tree
Reading state information... Done
All packages are up to date.

Step 2
------
install curl and apt-transport-https - APT transport for downloading via the HTTP Secure protocol (HTTPS).
This APT transport allows the use of repositories accessed via the HTTP Secure protocol
       (HTTPS), also referred to as HTTP over TLS.

ubuntu@nd-ctl-pl:~$ sudo apt -y install curl apt-transport-https
Reading package lists... Done
Building dependency tree
Reading state information... Done
curl is already the newest version (7.68.0-1ubuntu2.6).
curl set to manually installed.
The following NEW packages will be installed:
  apt-transport-https
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 4680 B of archives.
After this operation, 162 kB of additional disk space will be used.
Get:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/universe amd64 apt-transport-https all 2.0.6 [4680 B]
Fetched 4680 B in 0s (327 kB/s)
Selecting previously unselected package apt-transport-https.
(Reading database ... 91365 files and directories currently installed.)
Preparing to unpack .../apt-transport-https_2.0.6_all.deb ...
Unpacking apt-transport-https (2.0.6) ...
Setting up apt-transport-https (2.0.6) ...

Step 3
------

 Add Kubernetes repository for Ubuntu 20.04 to all the servers.

ubuntu@nd-ctl-pl:~$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
OK

ubuntu@nd-ctl-pl:~$ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list

deb https://apt.kubernetes.io/ kubernetes-xenial main

Step 4
------
Then install required packages.


ubuntu@nd-ctl-pl:~$ sudo apt update
Hit:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal InRelease
Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
Get:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-backports InRelease [101 kB]
Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease
Get:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [9383 B]
Get:6 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages [48.2 kB]
Fetched 272 kB in 1s (306 kB/s)
Reading package lists... Done
Building dependency tree
Reading state information... Done
All packages are up to date.


ubuntu@nd-ctl-pl:~$ sudo apt -y install vim git curl wget kubelet kubeadm kubectl

ubuntu@nd-ctl-pl:~$sudo apt-mark hold kubelet kubeadm kubectl

ubuntu@nd-ctl-pl:~$ kubectl version --client && kubeadm version
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.2", GitCommit:"8b5a19147530eaac9476b0ab82980b4088bbc1b2", GitTreeState:"clean", BuildDate:"2021-09-15T21:38:50Z", GoVersion:"go1.16.8", Compiler:"gc", Platform:"linux/amd64"}
kubeadm version: &version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.2", GitCommit:"8b5a19147530eaac9476b0ab82980b4088bbc1b2", GitTreeState:"clean", BuildDate:"2021-09-15T21:37:34Z", GoVersion:"go1.16.8", Compiler:"gc", Platform:"linux/amd64"}
Step 5
------

Disable swap 

ubuntu@nd-ctl-pl:~$ sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
ubuntu@nd-ctl-pl:~$ sudo swapoff -a


enable auto start in case system reboot -  overlay module for network and storage drivers
and containerd runtime for container handling

cat << EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

load the modlues now as as well

ubuntu@nd-ctl-pl:~$ sudo modprobe overlay

also br-netfilter - This module is required to enable transparent masquerading and to 
facilitate Virtual Extensible LAN (VxLAN) traffic for communication between Kubernetes
 pods across the cluster.

ubuntu@nd-ctl-pl:~$ sudo modprobe br_netfilter


enable  iptables filtering for bridge networking

Ref:

https://kubernetes.io/docs/setup/production-environment/container-runtimes/

ubuntu@nd-ctl-pl:~$ 

# Setup required sysctl params, these persist across reboots.
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

update the settings 

ubuntu@nd-ctl-pl:~$ sudo sysctl --system
* Applying /etc/sysctl.d/10-console-messages.conf ...
kernel.printk = 4 4 1 7
* Applying /etc/sysctl.d/10-ipv6-privacy.conf ...
net.ipv6.conf.all.use_tempaddr = 2
net.ipv6.conf.default.use_tempaddr = 2
* Applying /etc/sysctl.d/10-kernel-hardening.conf ...
kernel.kptr_restrict = 1
* Applying /etc/sysctl.d/10-link-restrictions.conf ...
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/10-magic-sysrq.conf ...
kernel.sysrq = 176
* Applying /etc/sysctl.d/10-network-security.conf ...
net.ipv4.conf.default.rp_filter = 2
net.ipv4.conf.all.rp_filter = 2
* Applying /etc/sysctl.d/10-ptrace.conf ...
kernel.yama.ptrace_scope = 1
* Applying /etc/sysctl.d/10-zeropage.conf ...
vm.mmap_min_addr = 65536
* Applying /usr/lib/sysctl.d/50-default.conf ...
net.ipv4.conf.default.promote_secondaries = 1
sysctl: setting key "net.ipv4.conf.all.promote_secondaries": Invalid argument
net.ipv4.ping_group_range = 0 2147483647
net.core.default_qdisc = fq_codel
fs.protected_regular = 1
fs.protected_fifos = 1
* Applying /usr/lib/sysctl.d/50-pid-max.conf ...
kernel.pid_max = 4194304
* Applying /etc/sysctl.d/99-cloudimg-ipv6.conf ...
net.ipv6.conf.all.use_tempaddr = 0
net.ipv6.conf.default.use_tempaddr = 0
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/kubernetes.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
* Applying /usr/lib/sysctl.d/protect-links.conf ...
fs.protected_fifos = 1
fs.protected_hardlinks = 1
fs.protected_regular = 2
fs.protected_symlinks = 1
* Applying /etc/sysctl.conf ...

Step 6
------

Containerd is an industry-standard container runtime with an emphasis on simplicity, robustness and portability

Docker – A developer-oriented software with a high level interface that lets you easily build and run containers
 from your terminal. It now uses containerd as its container runtime.

Containerd – An abstraction of kernel features that provides a relatively high level container interface. 
Other software projects can use this to run containers and manage container images.

Kubernetes – A container orchestrator that works with multiple container runtimes, including containerd , CRI-O.
 Kubernetes is focused on deploying containers in aggregate across one or more physical “nodes.” from the past, 
Kubernetes was tied to Docker. Kubernetes is deprecating Docker as a container runtime after v1.20. 
When Docker runtime support is removed in a future release (currently planned for the 1.22 release in late 2021) 
of Kubernetes it will no longer be supported and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports the docker daemon configurations you currently 
use (e.g. logging)


https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/

below extracted 

"Docker support in the kubelet is now deprecated and will be removed in a future release. 
The kubelet uses a module called "dockershim" which implements CRI support for Docker and 
it has seen maintenance issues in the Kubernetes community. We encourage you to evaluate moving
 to a container runtime that is a full-fledged implementation of CRI (v1alpha1 or v1 compliant API versions) as 
they become available. (#94624, @dims) [SIG Node]"

Let's install Containerd runtime
--------------------------------

step 6.1
--------
Ref : 
https://kubernetes.io/docs/setup/production-environment/container-runtimes/

ubuntu@ip-172-31-27-210:~$ sudo apt-get update && sudo apt-get install -y containerd

Hit:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal InRelease
Hit:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates InRelease
Hit:3 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-backports InRelease
Hit:4 http://security.ubuntu.com/ubuntu focal-security InRelease
Get:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [9383 B]
Fetched 9383 B in 0s (20.5 kB/s)
Reading package lists... Done
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
  runc
The following NEW packages will be installed:
  containerd runc
0 upgraded, 2 newly installed, 0 to remove and 110 not upgraded.
Need to get 37.0 MB of archives.
After this operation, 166 MB of additional disk space will be used.
Get:1 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 runc amd64 1.0.0~rc95-0ubuntu1~20.04.2 [4087 kB]
Get:2 http://us-east-2.ec2.archive.ubuntu.com/ubuntu focal-updates/main amd64 containerd amd64 1.5.2-0ubuntu1~20.04.3 [32.9 MB]
Fetched 37.0 MB in 1s (43.9 MB/s)
Selecting previously unselected package runc.
(Reading database ... 60246 files and directories currently installed.)
Preparing to unpack .../runc_1.0.0~rc95-0ubuntu1~20.04.2_amd64.deb ...
Unpacking runc (1.0.0~rc95-0ubuntu1~20.04.2) ...
Selecting previously unselected package containerd.
Preparing to unpack .../containerd_1.5.2-0ubuntu1~20.04.3_amd64.deb ...
Unpacking containerd (1.5.2-0ubuntu1~20.04.3) ...
Setting up runc (1.0.0~rc95-0ubuntu1~20.04.2) ...
Setting up containerd (1.5.2-0ubuntu1~20.04.3) ...
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service → /lib/systemd/system/containerd.service.
Processing triggers for man-db (2.9.1-1) ...

step 6.2
--------

ubuntu@ip-172-31-27-210:~$ sudo mkdir -p /etc/containerd

step 6.3
--------

generate the default configuration file and save it to a file called config.toml

toml - "Tom's Obvious, Minimal Language" referring to its creator, Tom Preston-Werner.


ubuntu@ip-172-31-27-210:~$ sudo containerd config default | sudo tee /etc/containerd/config.toml
disabled_plugins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd"
state = "/run/containerd"
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    enable_selinux = false
    enable_tls_streaming = false
    ignore_image_defined_volumes = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "k8s.gcr.io/pause:3.5"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      conf_template = ""
      max_conf_num = 1

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        runtime_engine = ""
        runtime_root = ""
        runtime_type = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = false

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        runtime_engine = ""
        runtime_root = ""
        runtime_type = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = ""

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]

    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"

  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.btrfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0

step 6.4
--------

Restart and check the status

ubuntu@ip-172-31-27-210:~$ sudo systemctl restart containerd

ubuntu@ip-172-31-27-210:~$ sudo systemctl status containerd
● containerd.service - containerd container runtime
     Loaded: loaded (/lib/systemd/system/containerd.service; enabled; vendor preset: enabled)
     Active: active (running) since Tue 2021-10-05 16:44:47 UTC; 14s ago
       Docs: https://containerd.io
    Process: 5579 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS)
   Main PID: 5588 (containerd)
      Tasks: 11
     Memory: 22.6M
     CGroup: /system.slice/containerd.service
             └─5588 /usr/bin/containerd

Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.875531443Z" level=info msg=serving...>
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.875599997Z" level=info msg=serving...>
Oct 05 16:44:47 nd-ctl-pl.tnh.com systemd[1]: Started containerd container runtime.
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.877473337Z" level=info msg="container>
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.879525679Z" level=info msg="Start sub>
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.879736122Z" level=info msg="Start rec>
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.879817478Z" level=info msg="Start eve>
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.879832569Z" level=info msg="Start sna>
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.879848264Z" level=info msg="Start cni>
Oct 05 16:44:47 nd-ctl-pl.tnh.com containerd[5588]: time="2021-10-05T16:44:47.879856619Z" level=info msg="Start str>

step 7
------
get the private IP address of the master node or the control plane node
and map it to the cluster name

ubuntu@nd-ctl-pl:~$ ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000
    link/ether 06:6f:bb:fa:9a:a2 brd ff:ff:ff:ff:ff:ff
    inet 172.31.19.229/20 brd 172.31.31.255 scope global dynamic eth0
       valid_lft 2746sec preferred_lft 2746sec
    inet6 fe80::46f:bbff:fefa:9aa2/64 scope link
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:80:5b:3e:c9 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

Set cluster endpoint DNS name or add record to /etc/hosts file.

ubuntu@nd-ctl-pl:~$ vi /etc/hosts
ubuntu@nd-ctl-pl:~$ sudo vi /etc/hosts

172.31.19.229 k8-cluster.tnh.com


step 8
------

Initiate the cluster in the control plane node

ubuntu@nd-ctl-pl:~$ sudo kubeadm init \
   --pod-network-cidr=192.168.0.0/16 \
   --control-plane-endpoint=k8-cluster.tnh.com
[init] Using Kubernetes version: v1.21.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8-cluster.tnh.com kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local nd-ctl-pl.tnh.com] and IPs [10.96.0.1 172.31.19.229]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost nd-ctl-pl.tnh.com] and IPs [172.31.19.229 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost nd-ctl-pl.tnh.com] and IPs [172.31.19.229 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.002742 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.21" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node nd-ctl-pl.tnh.com as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node nd-ctl-pl.tnh.com as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: bfrq2u.2pum44u33fcrixg2
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join k8-cluster.tnh.com:6443 --token bfrq2u.2pum44u33fcrixg2 \
        --discovery-token-ca-cert-hash sha256:ec23d58131a3a4ecd5872af3546cd1a74ddd7ab37c205991eee0980477aaa1cf \
        --control-plane

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8-cluster.tnh.com:6443 --token bfrq2u.2pum44u33fcrixg2 \
        --discovery-token-ca-cert-hash sha256:ec23d58131a3a4ecd5872af3546cd1a74ddd7ab37c205991eee0980477aaa1cf
ubuntu@nd-ctl-pl:~$


Step 9
------

Configure kubectl profile information  using commands in the output

ubuntu@nd-ctl-pl:~$  mkdir -p $HOME/.kube
ubuntu@nd-ctl-pl:~$   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
ubuntu@nd-ctl-pl:~$   sudo chown $(id -u):$(id -g) $HOME/.kube/config
ubuntu@nd-ctl-pl:~$ cd $HOME/.kube
ubuntu@master:~/.kube$ cd ..
ubuntu@nd-ctl-pl:~$ pwd
/home/ubuntu
ubuntu@nd-ctl-pl:~$ kubectl cluster-info
Kubernetes control plane is running at https://k8-cluster.tnh.com:6443
CoreDNS is running at https://k8-cluster.tnh.com:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

Step 10
-------

configure the calico pod network plugin. you can configure at your choice
https://kubernetes.io/docs/concepts/cluster-administration/addons/


ubuntu@nd-ctl-pl:~$ kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget
poddisruptionbudget.policy/calico-kube-controllers created
ubuntu@nd-ctl-pl:~$ watch kubectl get pods --all-namespaces

Step 11
-------

 Add the worker nodes

enter the IP of the control plane node in the etc/hosts file
ubuntu@nd-ctl-pl:~$ sudo vi /etc/hosts

172.31.19.229 k8-cluster.tnh.com

then continue to add the worker node having finished all the components installations including the containerd


ubuntu@nd-wk1:~$ sudo kubeadm join k8-cluster.tnh.com:6443 --token bfrq2u.2pum44u33fcrixg2 \
>         --discovery-token-ca-cert-hash sha256:ec23d58131a3a4ecd5872af3546cd1a74ddd7ab37c205991eee0980477aaa1cf
[preflight] Running pre-flight checks

[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

ubuntu@nd-wk1:~$


 All the nodes are up and running , the cluster is ready

ubuntu@nd-ctl-pl:/etc/kubernetes$ kubectl get no
NAME                STATUS   ROLES                  AGE   VERSION
nd-ctl-pl.tnh.com   Ready    control-plane,master   75m   v1.21.3
nd-wk1.tnh.com      Ready    <none>                 42m   v1.21.3
nd-wk2.tnh.com      Ready    <none>                 23m   v1.21.3

lets do a testing , on the control plane node, delploy a basic command pod

ubuntu@nd-ctl-pl:~$kubectl apply -f https://k8s.io/examples/pods/commands.yaml


ubuntu@nd-ctl-pl:~$ kubectl get po
NAME           READY   STATUS      RESTARTS   AGE
command-demo   0/1     Completed   0          52m
ubuntu@nd-ctl-pl:~$ kubectl describe po command-demo
Name:         command-demo
Namespace:    default
Priority:     0
Node:         nd-wk2.tnh.com/172.31.31.4
Start Time:   Fri, 30 Jul 2021 08:09:54 +0000
Labels:       purpose=demonstrate-command
Annotations:  cni.projectcalico.org/podIP:
              cni.projectcalico.org/podIPs:
Status:       Succeeded
IP:           192.168.233.1
IPs:
  IP:  192.168.233.1
Containers:
  command-demo-container:
    Container ID:  docker://e8e063a6a6cc29619c31169cb5ddab11fc05c7d13fe97d6e420bfcb77c03ebfe
    Image:         debian
    Image ID:      docker-pullable://debian@sha256:cc58a29c333ee594f7624d968123429b26916face46169304f07580644dde6b2
    Port:          <none>
    Host Port:     <none>
    Command:
      printenv
    Args:
      HOSTNAME
      KUBERNETES_PORT
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 30 Jul 2021 08:10:00 +0000
      Finished:     Fri, 30 Jul 2021 08:10:00 +0000
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2cgqc (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  kube-api-access-2cgqc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  53m   default-scheduler  Successfully assigned default/command-demo to nd-wk2.tnh.com
  Normal  Pulling    53m   kubelet            Pulling image "debian"
  Normal  Pulled     52m   kubelet            Successfully pulled image "debian" in 3.22785388s
  Normal  Created    52m   kubelet            Created container command-demo-container
  Normal  Started    52m   kubelet            Started container command-demo-container



also try 

kubectl describe node <node_name>

to check which container runtime the node uses.



